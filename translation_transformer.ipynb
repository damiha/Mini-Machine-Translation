{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformer import Transformer\n",
    "import time\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"deu.txt\", \"r\") as f:\n",
    "    text = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_phrases = []\n",
    "de_phrases = []\n",
    "\n",
    "n_phrases = 200_000\n",
    "\n",
    "max_en_len = float(\"-inf\")\n",
    "max_de_len = float(\"-inf\")\n",
    "\n",
    "index_max_en_len = -1\n",
    "index_max_de_len = -1\n",
    "\n",
    "for i, line in enumerate(text[:n_phrases]):\n",
    "    \n",
    "    parts = line.split(\"\\t\")\n",
    "    \n",
    "    en_phrase = parts[0]\n",
    "    de_phrase = parts[1]\n",
    "    \n",
    "    if len(en_phrase) > max_en_len:\n",
    "        max_en_len = len(en_phrase)\n",
    "        index_max_en_len = i\n",
    "  \n",
    "    if len(de_phrase) > max_de_len:\n",
    "        max_de_len = len(de_phrase)\n",
    "        index_max_de_len = i\n",
    "    \n",
    "    en_phrases.append(en_phrase)\n",
    "    de_phrases.append(de_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest English sentence\n",
      "EN: \"Have you seen Tom?\" \"Not in the last few days.\" (48)\n",
      "DE: „Hast du Tom gesehen?“ – „Nein, in den letzten Tagen gar nicht.“\n"
     ]
    }
   ],
   "source": [
    "print(\"The longest English sentence\")\n",
    "print(f\"EN: {en_phrases[index_max_en_len]} ({max_en_len})\")\n",
    "print(f\"DE: {de_phrases[index_max_en_len]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest German sentence\n",
      "EN: Mary is a soccer mom.\n",
      "DE: Maria ist als Mutter fast ausschließlich damit beschäftigt, ihre Kinder zu irgendwelchen Veranstaltungen zu kutschieren. (120)\n"
     ]
    }
   ],
   "source": [
    "print(\"The longest German sentence\")\n",
    "print(f\"EN: {en_phrases[index_max_de_len]}\")\n",
    "print(f\"DE: {de_phrases[index_max_de_len]} ({max_de_len})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, src_phrases, tgt_phrases):\n",
    "        \n",
    "        self.src_phrases = src_phrases\n",
    "        self.tgt_phrases = tgt_phrases\n",
    "        \n",
    "        chars_src = list(sorted(set(\"\\n\".join(src_phrases))))\n",
    "        chars_tgt = list(sorted(set(\"\\n\".join(tgt_phrases))))\n",
    "        \n",
    "        print(f\"The source texts contain {len(chars_src)} characters (+1 for padding)\")\n",
    "        print(f\"The target texts contain {len(chars_tgt)} characters (+1 for padding)\")\n",
    "        \n",
    "        # +1 since we are padding the sequences with the special token 0\n",
    "        self.ctoi_src = {c:(i + 1) for i, c in enumerate(chars_src)}\n",
    "        self.itoc_src = {(i + 1):c for i, c in enumerate(chars_src)}\n",
    "\n",
    "        self.ctoi_tgt = {c:(i + 1) for i, c in enumerate(chars_tgt)}\n",
    "        self.itoc_tgt = {(i + 1):c for i, c in enumerate(chars_tgt)}\n",
    "        \n",
    "        # to determine the size of the minimum positional encoding for encoder and decoder\n",
    "        \n",
    "        # encoder is the part that works on the source language\n",
    "        self.max_len_encoder = float(\"-inf\")\n",
    "        self.max_len_decoder = float(\"-inf\")\n",
    "        \n",
    "        for i, src_phrase in enumerate(src_phrases):\n",
    "            self.max_len_encoder = max(self.max_len_encoder, len(src_phrase))\n",
    "            self.max_len_decoder = max(self.max_len_decoder, len(tgt_phrases[i]))\n",
    "            \n",
    "        print(f\"Minimum encoder context size = {self.max_len_encoder}\")\n",
    "        print(f\"Minimum decoder context size = {self.max_len_decoder}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_phrases)\n",
    "    \n",
    "    def encode_src(self, s):\n",
    "        return [self.ctoi_src[c] for c in s]\n",
    "    \n",
    "    def decode_src(self, s):\n",
    "        return \"\".join([self.itoc_src.get(i, \"\") for i in s])\n",
    "    \n",
    "    def encode_tgt(self, s):\n",
    "        return [self.ctoi_tgt[c] for c in s]\n",
    "    \n",
    "    def decode_tgt(self, s):\n",
    "        # failures don't get printed\n",
    "        return \"\".join([self.itoc_tgt.get(i, \"\") for i in s])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        src_phrase = self.src_phrases[idx]\n",
    "        x = torch.tensor(self.encode_src(src_phrase))\n",
    "        \n",
    "        tgt_phrase = self.tgt_phrases[idx]\n",
    "        y = torch.tensor(self.encode_tgt(tgt_phrase))\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    sources, targets = zip(*batch)\n",
    "\n",
    "    # Pad the source and target sequences separately\n",
    "    sources_padded = pad_sequence(sources, batch_first=True, padding_value=0)\n",
    "    targets_padded = pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Optionally, you can also return the lengths of original sequences if your model needs them\n",
    "    #source_lengths = [len(src) for src in sources]\n",
    "    #target_lengths = [len(tgt) for tgt in targets]\n",
    "\n",
    "    return sources_padded, targets_padded #, source_lengths, target_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The source texts contain 93 characters (+1 for padding)\n",
      "The target texts contain 118 characters (+1 for padding)\n",
      "Minimum encoder context size = 48\n",
      "Minimum decoder context size = 120\n"
     ]
    }
   ],
   "source": [
    "token_dataset = TokenDataset(src_phrases = en_phrases, tgt_phrases = de_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = torch.utils.data.random_split(token_dataset, [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size, collate_fn = collate_fn)\n",
    "test_loader = DataLoader(test_set, shuffle=True, batch_size=batch_size, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from: https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(transformer, token_dataset, s, n_samples, n_tokens, beta):\n",
    "    \n",
    "    print(f\"Source: {s}\")\n",
    "    prompt_tokens = token_dataset.encode_src(s)\n",
    "    \n",
    "    response = transformer.sample(prompt_tokens, n_samples = n_samples,\n",
    "                                  n_tokens = n_tokens, beta = beta).cpu().numpy()\n",
    "    \n",
    "    print(\"Translation:\")\n",
    "    print(\"------------\")\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        \n",
    "        tokens = list(response[i])\n",
    "        translated = token_dataset.decode_tgt(tokens)\n",
    "        \n",
    "        print(f\"{i + 1}. {translated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(transformer, train_loader, val_loader, n_epochs,\n",
    "          optimizer=None,\n",
    "          lr_scheduler=None,\n",
    "          early_stopper=None,\n",
    "          metrics_per_epoch=10\n",
    "         ):\n",
    "    \n",
    "    transformer = transformer.to(device)\n",
    "    \n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.Adam(transformer.parameters(), lr=3e-4)\n",
    "        print(\"Using default optimizer\")\n",
    "        \n",
    "    if early_stopper is None:\n",
    "        early_stopper = EarlyStopper(patience=3, min_delta=1e-2)\n",
    "        print(\"Using default early stopper\")\n",
    "        \n",
    "    if lr_scheduler is None:\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                                  factor=0.3, patience=3, min_lr=1e-5,\n",
    "                                                                  threshold=1e-3\n",
    "                                                                 )\n",
    "        print(\"Using default LR scheduler\")\n",
    "    \n",
    "    # label smoothing deactivated for now\n",
    "    criterion_train = nn.CrossEntropyLoss(label_smoothing=0.0)\n",
    "    criterion_test = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_losses_over_epochs = []\n",
    "    val_losses_over_epochs = []\n",
    "    \n",
    "    metrics_every = len(train_loader) // metrics_per_epoch\n",
    "    in_between_epochs = []\n",
    "    in_between_metrics = []\n",
    "    \n",
    "    for epoch_idx in range(n_epochs):\n",
    "        \n",
    "        train_losses_this_batch = []\n",
    "        transformer.train()\n",
    "        \n",
    "        with tqdm(train_loader, desc=f\"Epoch {epoch_idx + 1}/{n_epochs}\", unit=\"batch\") as tepoch:\n",
    "            for batch_idx, (batch_src_seq, batch_tgt_seq) in enumerate(tepoch):\n",
    "                \n",
    "                B = batch_src_seq.shape[0]\n",
    "\n",
    "                # to GPU\n",
    "                batch_src_seq = batch_src_seq.to(device)\n",
    "                batch_tgt_seq = batch_tgt_seq.to(device)\n",
    "                \n",
    "                # the transformer gets the tokens delayed by one\n",
    "                dec_start_tokens = (torch.full((B, 1), fill_value=dec_start_token)).to(device)\n",
    "                decoder_input = torch.cat((dec_start_tokens, batch_tgt_seq), dim=1)[:, :-1]\n",
    "\n",
    "                logits = transformer(batch_src_seq, decoder_input)\n",
    "\n",
    "                logits = logits.transpose(1, 2)\n",
    "\n",
    "                loss = criterion_train(logits, batch_tgt_seq)\n",
    "\n",
    "                train_losses_this_batch.append(loss.item())\n",
    "\n",
    "                if batch_idx % metrics_every == 0:\n",
    "                    in_between_loss = np.mean(np.array(train_losses_this_batch)[-metrics_every:])\n",
    "                    in_between_metrics.append(in_between_loss)\n",
    "                    in_between_epochs.append(epoch_idx + (batch_idx / len(train_loader)))\n",
    "\n",
    "                    tepoch.set_postfix(avg_loss=in_between_loss)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        train_loss_this_epoch = np.mean(np.array(train_losses_this_batch))\n",
    "        train_losses_over_epochs.append(train_loss_this_epoch)\n",
    "        \n",
    "        # for early stopping\n",
    "        val_losses_this_batch = []\n",
    "        \n",
    "        transformer.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (batch_src_seq, batch_tgt_seq) in enumerate(tqdm(val_loader)):\n",
    "                \n",
    "                B = batch_src_seq.shape[0]\n",
    "\n",
    "                # to GPU\n",
    "                batch_src_seq = batch_src_seq.to(device)\n",
    "                batch_tgt_seq = batch_tgt_seq.to(device)\n",
    "                \n",
    "                # the transformer gets the tokens delayed by one\n",
    "                dec_start_tokens = (torch.full((B, 1), fill_value=dec_start_token)).to(device)\n",
    "                decoder_input = torch.cat((dec_start_tokens, batch_tgt_seq), dim=1)[:, :-1]\n",
    "\n",
    "                logits = transformer(batch_src_seq, decoder_input)\n",
    "\n",
    "                logits = logits.transpose(1, 2)\n",
    "\n",
    "                loss = criterion_train(logits, batch_tgt_seq)\n",
    "\n",
    "                val_losses_this_batch.append(loss.item())\n",
    "        \n",
    "        val_loss_this_epoch = np.mean(np.array(val_losses_this_batch))\n",
    "        val_losses_over_epochs.append(val_loss_this_epoch)\n",
    "        print(f\"{epoch_idx}. avg. train loss = {train_loss_this_epoch}, avg. val loss = {val_loss_this_epoch}\")\n",
    "        \n",
    "        should_stop = early_stopper.early_stop(val_loss_this_epoch)\n",
    "        lr_scheduler.step(val_loss_this_epoch)\n",
    "        \n",
    "        if should_stop:\n",
    "            print(f\"stopping early (val. loss did not decrease for {early_stopper.patience})\")\n",
    "            break\n",
    "        \n",
    "    return train_losses_over_epochs, in_between_epochs, in_between_metrics, val_losses_over_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from transformer_en_de_10e.pth\n"
     ]
    }
   ],
   "source": [
    "n_symbols_dec = 120\n",
    "\n",
    "# for the decoder during training and inference\n",
    "dec_start_token = n_symbols_dec - 1\n",
    "\n",
    "transformer = Transformer(d_model = 256,\n",
    "                          n_heads = 16,\n",
    "                          n_symbols_enc = 95, n_symbols_dec = n_symbols_dec,\n",
    "                          n_layers_enc=4,\n",
    "                          n_layers_dec=4, device=device, context_length_enc=128, context_length_dec=128)\n",
    "\n",
    "transformer.load_model(\"transformer_en_de_10e.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default early stopper\n",
      "Using default LR scheduler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|█████████| 1250/1250 [02:17<00:00,  9.08batch/s, avg_loss=0.509]\n",
      "100%|█████████████████████████████████████████| 313/313 [00:13<00:00, 23.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. avg. train loss = 0.5121127440214157, avg. val loss = 0.46505602803854895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|█████████| 1250/1250 [02:18<00:00,  9.03batch/s, avg_loss=0.503]\n",
      "100%|█████████████████████████████████████████| 313/313 [00:13<00:00, 23.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. avg. train loss = 0.5004833919763565, avg. val loss = 0.45865773602415577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses, in_between_epochs, in_between_metrics, val_losses =\\\n",
    "train(transformer, train_loader, test_loader, n_epochs=2, optimizer=optimizer, metrics_per_epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_list = np.arange(len(train_losses)) + 1.0\n",
    "plt.scatter(epoch_list, train_losses, label=\"train\", c=\"blue\")\n",
    "plt.plot(in_between_epochs, in_between_metrics, c=\"blue\")\n",
    "plt.plot(epoch_list, val_losses, label=\"val\", c=\"orange\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks(np.arange(len(train_losses) + 1))\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to transformer_en_de_10e.pth\n"
     ]
    }
   ],
   "source": [
    "#transformer.save_model(\"transformer_en_de_10e.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tom felt like going out for a walk.\n",
      "Tom hatte Lust auf einen Spaziergang.\n"
     ]
    }
   ],
   "source": [
    "example_idx = 0\n",
    "\n",
    "print(token_dataset.decode_src([c.item() for c in x[example_idx]]))\n",
    "print(token_dataset.decode_tgt([c.item() for c in y[example_idx]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: I am 22 years old.\n",
      "Prompt tokens: [34, 2, 52, 64, 2, 15, 15, 2, 76, 56, 52, 69, 70, 2, 66, 63, 55, 11]\n",
      "Translation:\n",
      "------------\n",
      "1. Ich verreunde meine Balle und in Balle.\n",
      "2. Ich und dann in im Biste um Parkaus gerade beschäftigt.\n",
      "3. Ich bin nur so berade bestanden, du sondern ich so spanzen w\n",
      "4. Ich bin gut 10 Vater Yollel und und meine Junge meine Fenn.\n",
      "5. Ich bin so so so gut mein Fahrrad neunden abbekluss.\n",
      "6. Ich bin deines nach Jahren immer ja gegen.\n",
      "7. Ich verlaube, ich bin meinem Tagen Freund zu besprechen.\n",
      "8. Ich bin meiner Jahre weit und Junge sprechen.\n"
     ]
    }
   ],
   "source": [
    "translate(transformer, token_dataset, s = \"I am 22 years old.\", n_samples = 8, n_tokens = 60, beta = 0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inpainting",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
